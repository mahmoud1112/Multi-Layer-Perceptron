{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import subprocess\nimport sys\n\n\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_csv_path_to_2d_list(path):\n    df = pd.read_csv(path)\n    return df.values\n\n\nif __name__ == '__main__':\n    training_split_ratio = 0.8\n\n    # read data from csv file.\n    df = read_csv_path_to_2d_list(\"/kaggle/input/spammmm/spam dataset.csv\")\n\n    # split the data into training and testing sets\n    train_set, test_set = train_test_split(df, train_size=training_split_ratio)\n\n    # collect the entire training text in a single 1d list\n    entire_training_text = train_set[:, 0]\n\n    # vectorize the text to numerical values based on its frequencies\n    vectorizer = CountVectorizer()\n    vectorizer.fit(entire_training_text)\n\n    # transform the training and testing set to numerical values using the vectorizer\n    train_set[:, 0] = [vectorizer.transform([text_i]).toarray().tolist()[0] for text_i in train_set[:, 0]]\n    test_set[:, 0] = [vectorizer.transform([text_i]).toarray().tolist()[0] for text_i in test_set[:, 0]]\n\n    # divide the data into x, and y to train over it.\n    x_train, y_train = train_set[:, 0].tolist(), train_set[:, 1].tolist()\n    x_test, y_test = test_set[:, 0].tolist(), test_set[:, 1].tolist()\n\n    # create the decision tree classifier model\n    model = DecisionTreeClassifier()\n    # fit the decision tree classifier model to the training data\n    model.fit(x_train, y_train)\n    # make predictions on the test data\n    y_pred1 = model.predict(x_test)\n\n    # create the multi-layer perceptron model (neural network)\n    model = MLPClassifier(verbose=True)\n    # fit the multi-layer perceptron model to the training data\n    model.fit(x_train, y_train)\n    # make predictions on the test data\n    y_pred2 = model.predict(x_test)\n\n    # compare the two models\n    # compute evaluation metrics for the decision tree classifier\n    accuracy1 = accuracy_score(y_test, y_pred1)\n    precision1 = precision_score(y_test, y_pred1)\n    recall1 = recall_score(y_test, y_pred1)\n    f1_1 = f1_score(y_test, y_pred1)\n\n    # compute evaluation metrics for the multi-layer perceptron model\n    accuracy2 = accuracy_score(y_test, y_pred2)\n    precision2 = precision_score(y_test, y_pred2)\n    recall2 = recall_score(y_test, y_pred2)\n    f1_2 = f1_score(y_test, y_pred2)\n\n    # compare the performance of the two classifiers\n    print(f'Decision tree classifier: Accuracy={accuracy1:.2f} Precision={precision1:.2f} Recall={recall1:.2f} F1={f1_1:.2f}')\n    print(f'Multi-layer Perceptron: Accuracy={accuracy2:.2f} Precision={precision2:.2f} Recall={recall2:.2f} F1={f1_2:.2f}')","metadata":{"_uuid":"f45c6780-c9ca-4c91-921a-a6cd9bc5dc3f","_cell_guid":"a0f08e9e-9f74-44cb-85b2-b650b6ee9898","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-18T23:55:15.388379Z","iopub.execute_input":"2022-12-18T23:55:15.388798Z","iopub.status.idle":"2022-12-18T23:57:57.856966Z","shell.execute_reply.started":"2022-12-18T23:55:15.388769Z","shell.execute_reply":"2022-12-18T23:57:57.856237Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Iteration 1, loss = 0.29707595\nIteration 2, loss = 0.07670511\nIteration 3, loss = 0.04162535\nIteration 4, loss = 0.02847483\nIteration 5, loss = 0.02083996\nIteration 6, loss = 0.01615500\nIteration 7, loss = 0.01281678\nIteration 8, loss = 0.01047085\nIteration 9, loss = 0.00870455\nIteration 10, loss = 0.00735383\nIteration 11, loss = 0.00631382\nIteration 12, loss = 0.00544917\nIteration 13, loss = 0.00477731\nIteration 14, loss = 0.00421957\nIteration 15, loss = 0.00376102\nIteration 16, loss = 0.00336701\nIteration 17, loss = 0.00304360\nIteration 18, loss = 0.00275325\nIteration 19, loss = 0.00252670\nIteration 20, loss = 0.00230936\nIteration 21, loss = 0.00212688\nIteration 22, loss = 0.00196374\nIteration 23, loss = 0.00182623\nIteration 24, loss = 0.00169694\nIteration 25, loss = 0.00158771\nIteration 26, loss = 0.00148792\nIteration 27, loss = 0.00139500\nIteration 28, loss = 0.00131593\nIteration 29, loss = 0.00124610\nIteration 30, loss = 0.00117532\nIteration 31, loss = 0.00111342\nIteration 32, loss = 0.00105961\nIteration 33, loss = 0.00100838\nIteration 34, loss = 0.00096334\nIteration 35, loss = 0.00091737\nIteration 36, loss = 0.00087684\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\nDecision tree classifier: Accuracy=0.94 Precision=0.88 Recall=0.92 F1=0.90\nMulti-layer Perceptron: Accuracy=0.97 Precision=0.96 Recall=0.93 F1=0.94\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}